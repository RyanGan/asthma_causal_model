---
title: "NO2 and asthma using TMLE"
author: "Ryan Gan"
date: "8/7/2021"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 2
---


## Introduction

NO2 is bad. We want to know if NO2 increases the likelihood of reporting an 
asthma ED visit in children with asthma living in Oakland. 

## Research Question

Overall Aim: Compare and contrast causal inference statistical methods to s
tandard regression and their utility in environmental epidemiology studies.

Standard Regression: Estimate the continuous relationship between increasing 
yearly average of NO~2~ and the likelihood of a reported ED visit in children 
with asthma. 

Causal Inference: Estimate the marginal effect of a defined binary exposure and 
the likelihood of a reported ED visit in children with asthma under the counter 
factual where everyone is exposed (Y=a1) and no one is expose (Y=a0).

<b>Study population:</b> Cohort of school children with asthma living in Oakland, 
California

<b>Outcome:</b> Emergency room visit

<b>Exposure:</b> NO~2~ exposure

### Setup

Loading libraries.

```{r packages, message=F, warning=F, echo=F}
library(tidyverse) # tidyverse 
library(glmnet) # lasso/elastic net
library(mgcv) # gam
# if necessary, install hdps `devtools::install_github("lendle/hdps")`
library(hdps) # high density propensity score algorithm
library(SuperLearner) # superlearner ensemble algorithm
library(xgboost) # reading in xgboost to use in superlearner
library(tmle) # caret ML processing
library(ctmle) # collaborative targeted maximum likelihood
library(caret) # data prep for ml models

# markdown options
knitr::opts_chunk$set(fig.height = 6, fig.width = 8, fig.align = "center",
                      progress = F, message = F, warning = F, echo=F)
```

Preparing Oakland Kicks Asthma data frame and joining with NO~2~ data frame.

```{r read_data}
# loading no2 value
no2layer <- haven::read_sas("./no2layer.sas7bdat") %>% # summary of NO2
  select(OKAID, NO2) %>% 
  mutate(OKAID = as.character(OKAID))
  
no2_col_names <- colnames(no2layer) %>% tolower() # lowercase all col names
colnames(no2layer) <- no2_col_names # assign lowercase col names to df

# loading oakland working
okawork <- haven::read_sas("./okawork1.sas7bdat") %>% 
  # convert okaid to character
  mutate(okaid = as.character(okaid))

# join okawork with no2layer by okaid to create final dataframe
asthma_no2_df <- okawork %>% right_join(no2layer, by = "okaid") %>% 
  # create ed visit variable from sh_11 var (assuming 1=event)
  mutate(edvisit = ifelse(sh_11 == 1, 1,
                   ifelse(sh_11 == 2, 0, NA))) %>% 
  # filter out missing edvisits
  filter(!is.na(edvisit)) %>%
  filter(asthma == 1) %>% 
  filter(sexmiss == 0 & racemiss == 0 & homelmiss == 0) %>% 
  # create no2 distributions
  mutate(no2_10dec = ifelse(no2 >= 19.9, 1, 0),
         no2_1qt = ifelse(no2 <= 16.237, 1, 0),
         no2_2qt = ifelse(no2 > 16.237 & no2 <= 17.180, 1, 0),
         no2_3qt = ifelse(no2 > 17.180 & no2 <= 18.090, 1, 0),
         no2_4qt = ifelse(no2 >= 18.090, 1, 0),
         no2_median = ifelse(no2 >= 17.180, 1, 0),
         age = as.numeric(round((timentry - dob)/365,0)),
         # imputate age based on grade
         age = ifelse(age < 10 & grade == 6, 11,
                  ifelse(is.na(age) & grade == 8, 14, age)))
```

Set up custom color theme.

```{r color_theme}
# move to source
ryan_theme <- theme(
  panel.background = element_rect(
    fill = "black", 
    colour = "black", 
    size = 0.5, 
    linetype = "solid"
    ),
  panel.grid.major = element_line(size = 0.2, colour = "white"),
  panel.grid.minor = element_line(size = 0.1, color = "white", linetype = "dotted"),
  plot.background = element_rect(fill = "#141e30", colour="#141e30", 
                                 size = 0.5, linetype = "solid"),
  text = element_text(colour = "white"),
  axis.text = element_text(colour = "white"),
  strip.background = element_blank(),
  strip.text = element_text(colour="white"),
  legend.background = element_blank(),
  legend.key = element_blank()
  )
```

## Study Population Description

The Oakland Kicks Asthma study is a school-based study with n=4017 children.

Our study population consists of 20% of these school-aged children (n=771) that
have a diagnosis of asthma living in Oakland. Data were collected from (dates). 

Our primary outcome is reported period prevalent emergency department visits in
the past year *(Sheryl is this correct?)*. We'll evaluate some basic demographic characteristics of this population by reported emergency department visit.

###Demographics by ED Visit 

```{r demo_tab}
# subset df to demographic variables
demo <- asthma_no2_df %>% 
  select(edvisit, age, grade, male, female, enghome, spanhome,
         chnshome,viethome, olanghome,api, afra, latino, white, natam, 
         mixed, othrace, mixhouse, pctlataian0, kidasnengn) %>% 
  # change some vars to factors
  mutate_at(vars(-age, - pctlataian0, -kidasnengn), as.factor) 


# colname demographic vector except edvisit
demo_vec <- colnames(demo)[c(-1,-2)]

# age distribution 
age_tab <- demo %>% 
  group_by(edvisit) %>% 
  summarise(mu_age = round(mean(age),1)) %>% 
  mutate(var = "age_yr_mu",
         edvisit = case_when(edvisit == 1 ~ 'yes_val',
                             edvisit == 0 ~ 'no_val')) %>% 
  spread(edvisit, mu_age)

# pctlataian and kidasn
cont_tab <- demo %>% 
  group_by(edvisit) %>% 
  summarise(mu_lat = mean(pctlataian0), med_lat = median(pctlataian0),
            mu_kidasnengn = mean(kidasnengn), med_kidasnengn = median(kidasnengn))

# n
age_tab <- demo %>% 
  group_by(edvisit) %>% 
  summarise(n=n()) %>% 
  mutate(var = "age_yr_mu",
         edvisit = case_when(edvisit == 1 ~ 'ed_yes_n',
                             edvisit == 0 ~ 'ed_no_n')) %>% 
  spread(edvisit, n) %>% 
  left_join(age_tab, by="var")

# precent table
prec_tab <- demo %>% 
  select(-edvisit, -age) %>% 
  map(~{as_tibble(round(prop.table(xtabs(~ .x + edvisit, data = demo),2)*100,
                        1))}) %>% 
  bind_rows(.id = "var") %>% 
  mutate(edvisit = case_when(edvisit == 1 ~ 'yes_val',
                             edvisit == 0 ~ 'no_val')) %>% 
  spread(edvisit, n) %>% 
  filter(.x == 1) %>% 
  select(-.x)

# n table
n_tab <- demo %>% 
  select(-edvisit, -age) %>% 
  map(~{as_tibble(xtabs(~ .x + edvisit, data = demo))}) %>% 
  bind_rows(.id = "var") %>% 
  mutate(edvisit = case_when(edvisit == 1 ~ 'ed_yes_n',
                             edvisit == 0 ~ 'ed_no_n')) %>% 
  spread(edvisit, n) %>% 
  filter(.x == 1) %>% 
  select(-.x)

# demo_tab
demo_tab <- n_tab %>% 
  left_join(prec_tab, by = "var") %>% 
  bind_rows(age_tab, .)

knitr::kable(demo_tab, 
  caption = "Demographic characterstics by ED Visit (most vals are percent, expect age, which is mean")
```

Again, some missingness in variables. It would be good to have a conversation on
if we should impute these. I will probably need to transform the dummy variables
to factor. 

## NO~2~ Distribution

Distribution of observed NO~2~ values.

```{r no2_dist}
no2_den_plot <- ggplot(asthma_no2_df, aes(x=no2)) +
  geom_density(color = '#9cecfb', fill = '#9cecfb', alpha = 0.7) +
  ylab("Density") +
  xlab(expression(paste("NO"[2], " ", mu,"g/m"^3))) +
  ryan_theme

# print no2 density plot
no2_den_plot
```

Assessing the relationship between NO~2~ and the proportion of ED visits by NO~2~ bins of 2 ug/m^3.

```{r no2_bin}
# checking linear relationship in p
no2_bin_df <- asthma_no2_df %>% 
  mutate(no2_bin = cut(no2, 
    breaks = seq(8, 24, by = 2), labels = seq(9, 23, by = 2))) %>% 
  group_by(no2_bin) %>% 
  summarise(n=n(), n_ed = sum(edvisit)) %>% 
  filter(!is.na(no2_bin)) %>% 
  mutate(ed_p = n_ed/n,
         no2 = as.numeric(as.character(no2_bin)),
         bin_size = log(n)/2)

# smooth spline fit
logit_smooth <- glm(edvisit ~ splines::ns(no2, 3), family = binomial(), 
                    asthma_no2_df) 

logit_lin <- glm(edvisit ~ no2, family = binomial(), asthma_no2_df) 

# predict based on no2 estimate
ns_ed <- predict(logit_smooth, data=asthma_no2_df, type = "response")
# ns_ed 95% CI around fit
ns_se <- predict(logit_smooth, data = asthma_no2_df, type = "response",
                      se.fit = T)$se.fit
# upper95 and lower95
ns_upper95 <- (ns_ed + 1.96*ns_se)
ns_lower95 <- (ns_ed - 1.96*ns_se)

# linear predictor
lin_ed <- predict(logit_lin, data = asthma_no2_df, type = "response")
# linear se
lin_se <- predict(logit_lin, data = asthma_no2_df, type = "response",
                      se.fit = T)$se.fit
# upper95 and lower95
lin_upper95 <- (lin_ed + 1.96*lin_se)
lin_lower95 <- (lin_ed - 1.96*lin_se)

# no2 vector
no2 <- asthma_no2_df$no2
# no2 ns spline
ed_ns_df <- data.frame(ns_ed, ns_lower95, ns_upper95, no2) %>% 
  rename(pr_ed = ns_ed, lower95 = ns_lower95, upper95 = ns_upper95) %>% 
  mutate(Fit = paste0("Spline (AIC: ", round(AIC(logit_smooth),1), ")"))

# lin fit
ed_lin_df <- data.frame(lin_ed, lin_lower95, lin_upper95, no2) %>% 
  rename(pr_ed = lin_ed, lower95 = lin_lower95, upper95 = lin_upper95) %>% 
  mutate(Fit = paste0("Linear (AIC: ", round(AIC(logit_lin),1), ")"))

# fit df
pr_ed_df <- bind_rows(ed_ns_df, ed_lin_df)

# plot of deciles by no2
fit_plot <- ggplot() +
  geom_point(data = no2_bin_df, aes(x = no2, y = ed_p, size = bin_size), 
             color = '#f2fcfe') +
  geom_line(data = pr_ed_df, aes(x=no2, y=pr_ed, group = Fit, color = Fit)) +
  geom_ribbon(data = pr_ed_df, aes(x=no2, ymin=lower95, ymax=upper95, 
                                   group = Fit, fill = Fit), alpha = 0.5) +
  scale_size(guide = F) +
  scale_color_manual(values = c('#1c92d2', '#e100ff')) +
  scale_fill_manual(values = c('#1c92d2', '#e100ff')) +
  facet_wrap(~Fit) +
  ylab("Proportion of Emergency \nDepartment Visits") +
  xlab(expression(paste("Mean of NO"[2], " ", mu,"g/m"^3, " Bin"))) +
  ryan_theme +
  theme(legend.direction = "horizontal", legend.position = "bottom")

# print fit plot
fit_plot
# save plot
#ggsave(filename = "fit_plot.png", fit_plot, width = 6, height = 3)
```


Fit by AIC suggests that for a logit model, a linear fit is just as good a fit 
as splines I've tested (I tried 2 knots and 3 knots but only showing 3 knot spline). 
I think it's fine to leave continuous NO~2~ as a linear predictor.

## Potential Confounders

There are over 437 variables. I believe Sheryl had identified confounders for 
the living < 500 meters of a roadway, but it's likely the potential confounders 
between NO~2~ cutoffs are different to the buffer, and perhaps even continous 
NO~2~. Individual schools are dummy-coded in so I'm going to take out the school
id variable.


<b>Note from Ryan 2021-10-19:</b>

In terms of possible confounder selection, is it still necessary to identify 
highly correlated variables for exclusion? Is the high-density propensity score 
necessary as well? 

The following chunk of code finds highly correlated variables above 0.9. I don't
think this step is as important in lasso or other regularized models, but leaving
it in since it's what I did originally.

```{r possible_covs}
# subset to possible covariates based off sheryl's list
all_covs <- asthma_no2_df %>% 
  select(inbuff, male, enghome:olanghome, api:othrace, pubhouse,
         assthouse, hillres:estuary, pwhite:pinst, pmchhwoc:pimmb80nc,
         psamehome95:pworkmom, pmomnlf:urban) %>% 
  rename_all(funs(str_replace(., "_", ".")))

# nearzero variance of possible covs
nzv <- nearZeroVar(all_covs)
# filter to covariates that do not have near zero variance
covs_w_var <- all_covs[, -nzv]
# from 339 to 273

# create covariance matrix using non-parametric spearman to find vars with high
# correlation
corr_mat <- cor(covs_w_var, method = "spearman")

# find highly correlated predictors
high_cor <- findCorrelation(corr_mat, cutoff = 0.9)  

# filter to variables with lower correlation than 0.8
possible_covs <- covs_w_var[,-high_cor] 
# reduced to 273 to 225

```

### High-Density Propensity Score

I'm using the high-density propensity score algorithm to further reduce the 
dimensions of covariates to only the 5 most influential. I like the high-density
propensity score because it considers the relationship between both exposure and
outcomes, and the possibility of that covariate for confounding. As we have many
questionnaire covariates, environmental covariates, and census demographic 
covariates, I like this data reduction approach instead of building models piece
by piece.

For simplicity, I'm going to fit the HDPS model based on the binary cutoff of 
NO~2~ at the upper decline of the NO~2~ distribution, which is a value of 
19.9 ug/m^3^. It looks like this will work based on the fit plots below. 
It also gives us the counter factual question:

What would the reported proportion of ED visits be in children with asthma if 
neighborhood NO~2~ were reduced below 19.9 ug/m^3^?

```{r hdps}
# trying out hdps to reduce to 50 vars
# step 2,3,4: identify covariates associated with treatment and outcome
hdps_vars <- hdps_screen(outcome = asthma_no2_df$edvisit, 
                         treatment = asthma_no2_df$no2_10dec,
                         covars = possible_covs, keep_n_per_dimension = 5,
                         keep_k_total = 10, verbose = T)

# output vector of covariates identified by HDS
hdps_vars_vec <- predict(hdps_vars) %>% 
  colnames() %>% 
  str_replace(., "_.*", "") %>% 
  unique()

# subset covariates to hdps_vars_vec
hdps_vars_df <- possible_covs[, colnames(possible_covs) %in% hdps_vars_vec]
# names
print(colnames(hdps_vars_df))
```

The HDPS identified the following potential confounders: 

Subject-specific binary indicators of male, African American, and resident of a mixed housing land use area. 

Census-level variables percentage of children in census block? that speak Asian or Pacific Islander language, but not English well. Also, percentage American Indian, Native American of Latino population by census tract.

I will include these in adjusted models. Note, I realize that what might confounded the binary cutoff of NO~2~ may be different from continuous NO~2~. I have not spent time identifying variables that may predict continuous NO~2~. This is a limitation, but I think it's minor.  

## Standard Models

### Continous Relationship

Linear model fits these data just as well as a spline with 3 degrees of freedom.
I'm going to estimate the linear relationship between a 1 ug/m^3^ increase in 
NO~2~ and the proportion/likelihood of reporting an asthma ED visit using both 
the identity link (difference in proportion) and the log link 
(relative difference in proportion).

```{r unadjusted_lin_relationships}
# unadjusted linear model
unadj_diff <- broom::tidy(glm(edvisit ~ no2, data=asthma_no2_df, 
             family = binomial(link="identity"))) %>% 
  filter(term == "no2") %>% 
  select(estimate, std.error) %>% 
  mutate(model = 'Logistic',
         type = "Difference",
         Y = "ED Visit",
         A = "NO2 continous",
         W = "Unadjusted",
         lower95 = round(estimate - 1.96*std.error,3),
         upper95 = round(estimate + 1.96*std.error,3),
         estimate = round(estimate, 3)) %>% 
  select(model, type, Y, A, W, estimate, lower95, upper95)

# relative
# unadjusted linear model
unadj_or <- broom::tidy(glm(edvisit ~ no2, data=asthma_no2_df, 
             family = binomial(link="log"))) %>% 
  filter(term == "no2") %>% 
  select(estimate, std.error) %>% 
  mutate(model = 'Logistic',
         type = "Odds Ratio",
         covariate = 'Unadjusted',
         Y = "ED Visit",
         A = "NO2 continous",
         W = "Unadjusted",
         lower95 = round(exp(estimate - 1.96*std.error),3),
         upper95 = round(exp(estimate + 1.96*std.error),3),
         estimate = round(exp(estimate), 3)) %>% 
  select(model, type, Y, A , W, estimate, lower95, upper95)

# table
knitr::kable(bind_rows(unadj_diff, unadj_or), 
  caption = 'Unadjusted linear relationship between NO2 and ED Visits')
```

Adjusted models adjusting for HDPS covariates. Note, I should really consider iterating for each model scenario.

```{r adj_linear_relationship}
# adjusted linear model
adj_diff <- broom::tidy(glm(edvisit ~ no2 + male + afra + mixhouse + 
                              pctlataian0 + kidasnengn, data=asthma_no2_df, 
             family = binomial(link="identity"))) %>% 
  filter(term == "no2") %>% 
  select(estimate, std.error) %>% 
  mutate(model = 'Logistic',
         type = "Difference",
         Y = "ED Visit",
         A = "NO2 continous",
         W = "HDPS",
         lower95 = round(estimate - 1.96*std.error,3),
         upper95 = round(estimate + 1.96*std.error,3),
         estimate = round(estimate, 3)) %>% 
  select(model, type, Y, A, W, estimate, lower95, upper95)

# adjusted linear model
adj_or <- broom::tidy(glm(edvisit ~ no2 + male + afra + mixhouse + 
            pctlataian0 + kidasnengn, data=asthma_no2_df, 
            family = binomial(link="log"))) %>% 
  filter(term == "no2") %>% 
  select(estimate, std.error) %>% 
  mutate(model = 'Logistic',
         type = "Odds Ratio",
         covariate = 'Unadjusted',
         Y = "ED Visit",
         A = "NO2 continous",
         W = "HDPS",
         lower95 = round(exp(estimate - 1.96*std.error),3),
         upper95 = round(exp(estimate + 1.96*std.error),3),
         estimate = round(exp(estimate), 3)) %>% 
  select(model, type, Y, A , W, estimate, lower95, upper95)

# table
knitr::kable(bind_rows(adj_diff, adj_or), 
  caption = 'Adjusted linear relationship between NO2 and ED Visits')
```

Adjusted variables differ very little from crude estimates.

### Binary Cutoff

Causal inference models like Targeted Maximum Likelihood (TMLE) generally require a binary point exposure, although there are exceptions when the relationship between A and Y is linear (g-estimation, structural equations modeling). I do believe there are extensions of TMLE now for a bounded continous A variable, but I have not really looked in to these methods yet.

```{r adj_linear_relationship_binary}
# adjusted linear model
adj_diff_binary <- broom::tidy(glm(edvisit ~ no2_10dec + male + afra + mixhouse + 
                              pctlataian0 + kidasnengn, data=asthma_no2_df, 
             family = binomial(link="identity"))) %>% 
  filter(term == "no2_10dec") %>% 
  select(estimate, std.error) %>% 
  mutate(model = 'Logistic',
         type = "Difference",
         Y = "ED Visit",
         A = "NO2 > 19.9",
         W = "HDPS",
         lower95 = round(estimate - 1.96*std.error,3),
         upper95 = round(estimate + 1.96*std.error,3),
         estimate = round(estimate, 3)) %>% 
  select(model, type, Y, A, W, estimate, lower95, upper95)

# unjusted linear model
unadj_or_binary <- broom::tidy(glm(edvisit ~ no2_10dec, data=asthma_no2_df, 
            family = binomial(link="log"))) %>% 
  filter(term == "no2_10dec") %>% 
  select(estimate, std.error) %>% 
  mutate(model = 'Logistic',
         type = "Odds Ratio",
         covariate = 'Unadjusted',
         Y = "ED Visit",
         A = "NO2 > 19.9",
         W = "Unadjusted",
         lower95 = round(exp(estimate - 1.96*std.error),3),
         upper95 = round(exp(estimate + 1.96*std.error),3),
         estimate = round(exp(estimate), 3)) %>% 
  select(model, type, Y, A , W, estimate, lower95, upper95)

# adjusted linear model
adj_or_binary <- broom::tidy(glm(edvisit ~ no2_10dec + male + afra + mixhouse + 
            pctlataian0 + kidasnengn, data=asthma_no2_df, 
            family = binomial(link="log"))) %>% 
  filter(term == "no2_10dec") %>% 
  select(estimate, std.error) %>% 
  mutate(model = 'Logistic',
         type = "Odds Ratio",
         covariate = 'HDPS',
         Y = "ED Visit",
         A = "NO2 > 19.9",
         W = "HDPS",
         lower95 = round(exp(estimate - 1.96*std.error),3),
         upper95 = round(exp(estimate + 1.96*std.error),3),
         estimate = round(exp(estimate), 3)) %>% 
  select(model, type, Y, A , W, estimate, lower95, upper95)

# table
knitr::kable(bind_rows(unadj_or_binary, adj_or_binary), 
  caption = 'Adjusted relationship between NO2 >19.9 and ED Visits')
```

## Targeted Maximum Likelihood Estimation

Using TMLE to estimate the marginal effect of reducing NO~2~ levels below 19.9 on reported ED visit.

Setting up counterfactual estimates of Q formula. The following code chunk is for collaborative TMLE, but I don't think I'll go that route since I've reduced the dimensions of possible covariates to 5 now.

```{r scalable_ctmle}
# length of observations
n <- length(asthma_no2_df$edvisit)
# observed Y
Y <- asthma_no2_df$edvisit
# observed X
A <- asthma_no2_df$no2_10dec
# covariates
W <- as.matrix(hdps_vars_df)
# start by building initial estimate of Q
# counterfactual estimate of Y given all exposure is 0
Ya0 <- rep(mean(Y[A == 0]),n)
# counterfactual estimate of Y given all exposure is 1
Ya1 <- rep(mean(Y[A == 1]),n)
# assign initial estimate of Q e
Q <- cbind(Ya0, Ya1)

```


Running a standard TMLE model. Added XGBoost model in as well.

```{r tmle_model}
# sl lib
sl_lib <- c("SL.glmnet", "SL.glm", "SL.step", "SL.xgboost")
# tmle model
tmle_mod <- tmle(
  Y = Y, A = A, W = W, 
  Q.SL.library = sl_lib, g.SL.library = sl_lib, 
  family = "binomial", V = 10
  )
```

Checking propensity of NO~2~ exposure from TMLE model for ETA violation. 
Not the greatest distribution, but no probabilities of 0 or 1.

```{r tmle_g_eta}
# predict on test
g_pred <- as_data_frame(tmle_mod$g$g1W) %>% 
  rename(g = value) 


# histogram of predicted values
eta_plot <- ggplot(data.frame(g_pred), aes(x=g)) +
  geom_histogram(fill = "#bdfff3", color = "#bdfff3", alpha = 0.5, bins = 50) +
  xlim(0, 1) +
  ylab('Density') +
  xlab(expression(paste("Probability of NO"[2], " >19.9 ", mu,"g/m"^3, " | W"))) +
  ryan_theme


# print eta plot
eta_plot
# save plot
#ggsave(filename = "eta_plot.png", eta_plot, width = 6, height = 3)
```

TMLE Estimates.

```{r tmle_est}
tmle_mod 
```

### TMLE Model 2 

Using all possible covariates. Should be okay with glmnet and xgboost.

```{r tmle_all_covs}
# updating SL libraries taking out glm and step (not great for high dim data)
sl_lib2 <- c("SL.glmnet", "SL.xgboost")
# new model covariate matrix with all variables
W2 <- as.matrix(all_covs)

# run new model
# tmle model
tmle_mod2 <- tmle(
  Y = Y, A = A, W = W2, 
  Q.SL.library = sl_lib2, g.SL.library = sl_lib2, 
  family = "binomial", V = 5
  )

```

Check updated g estimate (propensity) from TMLE model 2 with all variables.



```{r tmle_g_eta2}
# predict on test
g_pred2 <- as_data_frame(tmle_mod2$g$g1W) %>% 
  rename(g = value) 


# histogram of predicted values
eta_plot2 <- ggplot(data.frame(g_pred2), aes(x=g)) +
  geom_histogram(fill = "#bdfff3", color = "#bdfff3", alpha = 0.5, bins = 50) +
  xlim(0, 1) +
  ylab('Density') +
  xlab(expression(paste("Probability of NO"[2], " >19.9 ", mu,"g/m"^3, " | W"))) +
  ryan_theme


# print eta plot
eta_plot2
# save plot
#ggsave(filename = "eta_plot.png", eta_plot, width = 6, height = 3)
```


```{r tmle_est2}
tmle_mod2 
```



## Collaborative TMLE

I've run this, but it won't improve on TMLE since I've reduce variables to only
5 and made sure they balance the propensity. Consider coming back to this with 
more dimensions for the paper.

Building initial lasso fit of g formula.

```{r g_form}
# lasso fit
lasso_fit <- cv.glmnet(y=A, x=W, family = "binomial", nlambda =100, 
                       nfolds = 5)
```

Build sequence of lmabdas from lambda selected by CV.

```{r lasso_lambdas}
lasso_lambdas <- lasso_fit$lambda[lasso_fit$lambda <=
                                  lasso_fit$lambda.min][1:10]
```

Custom lasso superlearner library.

```{r sl_lasso_template}
# Build SL template for glmnet
SL.glmnet_new <- function(Y, X, newX, family, obsWeights, id, alpha = 1,
                           nlambda = 100, lambda = 0,...){
      # browser()
      if (!is.matrix(X)) {
            X <- model.matrix(~-1 + ., X)
            newX <- model.matrix(~-1 + ., newX)
      }
      fit <- glmnet::glmnet(x = X, y = Y,
                            lambda = lambda,
                            family = family$family, alpha = alpha)
      pred <- predict(fit, newx = newX, type = "response")
      fit <- list(object = fit)
      class(fit) <- "SL.glmnet"
      out <- list(pred = pred, fit = fit)
      return(out)
}

# Use a sequence of estimator to build gn sequence:
SL.cv1lasso <- function (... , alpha = 1, lambda = lasso_lambdas[1]){
      SL.glmnet_new(... , alpha = alpha, lambda = lambda)
}

SL.cv2lasso <- function (... , alpha = 1, lambda = lasso_lambdas[2]){
      SL.glmnet_new(... , alpha = alpha, lambda = lambda)
}

SL.cv3lasso <- function (... , alpha = 1, lambda = lasso_lambdas[3]){
      SL.glmnet_new(... , alpha = alpha, lambda = lambda)
}

SL.cv4lasso <- function (... , alpha = 1, lambda = lasso_lambdas[4]){
      SL.glmnet_new(... , alpha = alpha, lambda = lambda)
}

SL.library = c('SL.cv1lasso', 'SL.cv2lasso', 'SL.cv3lasso', 'SL.cv4lasso', 'SL.glm')
```

Construct the object folds, which is a list of indices for each fold.

```{r lasso_folds}
V = 5
folds <- by(sample(1:n,n), rep(1:V, length=n), list)
```

Use folds and SuperLearner template to compute gn_candidates and gn_candidates_cv.

```{r gn_seq}
gn_seq <- build_gn_seq(A = A, W = W, SL.library = SL.library, folds = folds)
```

C-TMLE only has difference estiamtes right now. I looked at the function and believe I can modify it if we decided to estimate the ratio.

```{r ctmle_fit}
# run ctmle model
ctmle_general_fit1 <- ctmleGeneral(Y = Y, A = A, W = W, Q = Q,
                                   ctmletype = 1, family = "binomial",
                                   gn_candidates = gn_seq$gn_candidates,
                                   gn_candidates_cv = gn_seq$gn_candidates_cv,
                                   folds = folds, V = 5)
# print fit
ctmle_general_fit1
```

## Comparison of Binary Estimates Using Standard Model vs. TMLE

```{r results_tab}
# tmle_results
tmle_or <- round(tmle_mod$estimates$OR$psi,3)
tmle_lower95 <- round(tmle_mod$estimates$OR$CI, 3)[1]
tmle_upper95 <- round(tmle_mod$estimates$OR$CI, 3)[2]

# add tmle 2 with all covariates
tmle2_or <- round(tmle_mod2$estimates$OR$psi,3)
tmle2_lower95 <- round(tmle_mod2$estimates$OR$CI, 3)[1]
tmle2_upper95 <- round(tmle_mod2$estimates$OR$CI, 3)[2]

# tmle_results 
tmle_results <- data.frame(tmle_or, tmle_lower95, tmle_upper95) %>%
  rename(estimate = tmle_or, lower95 = tmle_lower95, upper95 = tmle_upper95) %>% 
  mutate(model = "TMLE", type = "Odds Ratio", Y = "ED Visit", A = "NO2 > 19.9",
         W = "HDPS")

# tmle 2 results
tmle2_results <- data.frame(tmle2_or, tmle2_lower95, tmle2_upper95) %>%
  rename(estimate = tmle2_or, lower95 = tmle2_lower95, upper95 = tmle2_upper95) %>% 
  mutate(model = "TMLE", type = "Odds Ratio", Y = "ED Visit", A = "NO2 > 19.9",
         W = "All possible covariates")

relative_results <- bind_rows(
  unadj_or_binary, adj_or_binary, tmle_results, tmle2_results
  ) %>% 
  mutate(Model = factor(paste0(model, ": ", W), 
    levels = c("Logistic: Unadjusted", "Logistic: HDPS", 
               "TMLE: HDPS", "TMLE: All possible covariates")))

# table
knitr::kable(relative_results, 
  caption = 'Results for different models between NO2 >19.9 and ED Visits')
```

Plot of same results.

```{r results_plot}
# results plot
results_plot <- ggplot(data = relative_results, aes(x = Model, y = estimate,
                                                    color = model, group = model)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymin=lower95, ymax=upper95), width = 0.2) +
  scale_color_manual("Model Type", values = c("#9cecfb", "#ff00cc")) +
  geom_hline(yintercept = 1, linetype = "dashed", colour = "red") +
  ylim(0.8, 4) +
  ylab(expression(paste("Odds Ratio: NO"[2], " > 19.9 ", mu,"g/m"^3))) +
  ryan_theme +
  theme(axis.title.x = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_line(linetype = "dotted"),
        panel.grid.minor = element_blank(),
        legend.direction = "horizontal", legend.position = "bottom")
# print results
results_plot
# save plot
#ggsave(filename = "result_plot.png", results_plot, width = 6, height = 3)
```

## Conclusion

We implemented a standard and causal inference framework to assess the 
relationship between NO2 and ED visits and found strengths and limitations to 
both approaches. The standard modeling approach allowed us to evaluate the
concentration-response relationship between the exposure and outcome, but 
these results are harder to translate in to actionable policy or public health
decisions. In contrast, results from our TMLE formalizes our counter factual 
question on the population effect if we reduced NO2 below 19.9 µg/m^3^. However, 
this association cannot be interpreted causally due to violations in conditional
exchangeability and the cross-sectional nature of NO~2~ assessment and reported 
ED visits.


## Continuous models 

Added 2021-10-19. Reason for trying these models is that the >19.9 µg/m^3^ NO~2~
cutoff is based on the proportion of ED visits plot ablove taht looksn like 
there is an elevated proportion based on bins. 

I'll try some other approaches that leaves NO~2~ as continuous.

First, I'll try a linear lasso while making sure to not penalize NO~2~. 

Note event a small penalty shrinks NO~2~ coef to 0. Suggests to me that there 
isn't much of an effect of NO~2~. 

```{r lasso}

# pull out no2 vector
no2 <- asthma_no2_df$no2
# set as matrix
mat <- as.matrix( cbind( no2, all_covs ) )
# need to set penalty factor alpha
# 0 l1 penalty for no2, 0.5 for the rest
pen_factor <- c(0, rep(0.5, ncol(mat) - 1))

set.seed(123)
no2_glmnet <- cv.glmnet( 
  x =  mat, 
  y = asthma_no2_df$edvisit, 
  family = "binomial", 
  #type.measure = "auc", 
  nfolds = 5, 
  penalty.factor = pen_factor
  )

# get lasso coef
lasso_c <- coef(no2_glmnet) 

print(paste0(
  'Odds ratio from elastic net (lasso penalty): ', 
   round( exp( lasso_c[which(row.names(lasso_c) == 'no2')] ) , 3 )
  )
  )
```


Testing non-linearity with penalized generalized additive models rather than 
binned proportions. With GLMNET loaded I think it loads it's own GAM package. I 
want to use Simon Woods mgcv for GAMs.

Is annual average NO~2~ and prevalent ED visits linear? Starting with a model
only with NO~2~ in it with a smoothing term of a cubic regression spline with 
10 knots.

Summary of GAM model suggests effective degrees of freedom is around 5, and
that term might be meaningful (based on p-value from smooth terms).

```{r no2_gam}
gam <- mgcv::gam(edvisit ~ s(no2), family = binomial(), data = asthma_no2_df)

# summary from gam model, smooth term has around 
print( summary(gam) )

```

Plotting smooth term of NO~2~. Looking at the plot, looks like kids that live in
lower average annual NO~2~ areas have a lower likelihood of reporting 
a prevalent ED visit during that year with wide variability (low number of kids),
that increases to maybe around 12 ug/m^3^ that then plateaus. Based on the 
penalized GAM there might be a non-linear association. I saw this trend in the 
binned prevalence plots above, but this is probably a more principled way to 
look for non-linearities.

```{r no2_smooth_plot}

plot(gam, seWithMean = TRUE)

```

I'll try a quick identificaiton of some confounding variables using lasso to 
find coefficients that predict exposrue and outcome and the overlap of those
two models will make up the confounding adjustment variables.

```{r confounder_adjusted_gam}
# variable selection via glmnet for outcomeß
set.seed(123)
glmnet1 <- cv.glmnet( 
  x =  as.matrix(all_covs), 
  y = asthma_no2_df$edvisit, 
  family = "binomial", 
  #type.measure = "auc", 
  nfolds = 5, 
  alpha = 0.5
  )

# coef that predict outcome
c_outcome <- coef(glmnet1, s = 'lambda.min', exact = TRUE )

inds <- which(c_outcome != 0)
outcome_variables <- row.names(c)[inds]
outcome_variables <- outcome_variables[!( outcome_variables %in% '(Intercept)')]

# exposure variable
set.seed(123)
glmnet2 <- cv.glmnet( 
  x =  as.matrix(all_covs), 
  y = asthma_no2_df$no2, 
  family = "gaussian", 
  type.measure = "mse", 
  nfolds = 5, 
  alpha = 0.5
  )

# find covs associated with no2
c_no2 <- coef(glmnet2, s = 'lambda.min', exact = TRUE )

inds <- which(c_no2 != 0)
no2_variables <- row.names(c)[inds]
no2_variables <- no2_variables[!( no2_variables %in% '(Intercept)')]



# find variables in both outcome and no2
confounder_vars <- outcome_variables[outcome_variables %in% no2_variables]

print( 
  paste0(
    'Variables identified as possible confounders via lasso: ', 
    paste(confounder_vars, collapse = ', ') 
    ) 
  )

# create dataframe of selected variables
select_vars <- all_covs[, confounder_vars]

```

Based on selected variables that are associated with both NO~2~ and ED visits
I'll add those to the GAM. K

Kidieeng: Percentage that speak other Indo-European language and English very 
well or well at home out of children ages 5 to 17 years by census tract. Treated
as smooth term.

pubhouse: Indicator variable for public housing. 

assthouse: Indicator variable for assisted housing.

hillres: Resident of Hill land use area.

migrhsing: Percentage for migrant workers out of all vacant housing units by 
census tract.

blackmaj: Listed in the race section of the ecovars file. Indicator variable for
Black/African American population majority (per census tract?)

```{r gam2}
# fit gam with selected vars
gam2 <- mgcv::gam(
  edvisit ~ 
    # no2
    s(no2) +
    # smooth terms
    s(kidieengg) + 
    # fixed
    pubhouse + assthouse + hillres + migrhsing + blackmaj, 
  family = binomial(), 
  data = asthma_no2_df
  )

# summary of gam
print( summary(gam2) )

```

Based on the summary of the GAM with adjustment variables, public housing,
assisted housing, and percent of migrant workers out of all vacant housing 
increase the likelihood of ED visits, where hill residence decreases likelihood
of ED visits. For the smooth terms, NO~2~ is penalized to linear based on EDF of
1 and p-value suggests no effect on ED visits. For proportion of kids that speak
english well by census tract, it's been penalized to linear as well based on 
EDF of 1, but p suggests a signficant effect. 

Plotting smooths of no2 and kidieengg below. Conditioning on these variables
identified to be associated with NO~2~ and ED visits, NO~2~ looks to have no
effect at any level. Kidieengg looks to decrease likelihood of ED visits as
the value increases.

```{r gam2_plot}

plot(gam2, pages = 1, seWithMean = TRUE)


```


### Things to consider:

<b>Continuous vs. binary NO~2~:</b>
- Is the association between NO~2~ and likelihood of ED visits linear?

- Loss of precision going from continuous to binary

- Binary easier to ask public health-relevant questions 
(i.e. what if we reduce below a certain NO~2~ amount?)

- What's an appropriate binary cutoff if so? Eye-ball of >19? Upper 90th 
percentile? Annual average NO~2~? For all these kids looks to be below EPA 
standards. 
